{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhkpffGffGov"
   },
   "source": [
    "#Name: Joseph Ereifej\n",
    "##ECE2975 Reinforcement Learning 2023 - Homework Assignment 1: MDPs and Bellman Equations\n",
    "\n",
    "Total: 100 points\n",
    "\n",
    "Topics covered on this Assignment:\n",
    "\n",
    "Markov Decission Processes (MDPs)\n",
    "\n",
    "Probabilities of transition in T steps\n",
    "Example: \"Student MDP\"\n",
    "Dinamic programming:\n",
    "\n",
    "Policy Improvement Theorem\n",
    "Policy Evaluation\n",
    "Policy Iteration\n",
    "Value Iteration\n",
    "Completion of this assignment entails answering questions and writing code sections.\n",
    "\n",
    "Suggested lecture: Sutton & Barto 2020 Caps. 3 y 4 (link)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oe-AYvi4231C"
   },
   "source": [
    "# Libraries to load\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYX6RQDUc-pc"
   },
   "source": [
    "# 1. T-step state probabilities (25pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqSyfwAhd319"
   },
   "source": [
    "## 1.1\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(S_T = s^\\prime | S_0 = s)=\\\\\n",
    "\\end{equation}\n",
    "if T == 1:\n",
    "\\begin{equation}\n",
    "\\sum P(S_T = s^\\prime | A_0 = A_i) \\pi (A_i | S_0)\n",
    "\\end{equation}\n",
    "else\n",
    "\\begin{equation}\n",
    "\\sum P(S_T = s^\\prime | S_{T-1} = S_i, A_{T-1} = A_j) \\pi (A_j | S_i)*P(S_{T-1} = s_i | S_0 = s)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Briefly explain your response:\n",
    "    This is a recursive solution for the generalized probability of being in a final state S' given and initial state S0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn2sGTt8fLYb"
   },
   "source": [
    "## 1.2\n",
    "\n",
    "$$P(\\mathcal T_3=\\texttt{happy,books,happy,books,sad,party,sad})=0.004*P(S_0)$$\n",
    "\n",
    "Briefly explain your response:\n",
    "![](ECE2795HW1%20Pr%201.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcBXRhBY-Nkl"
   },
   "source": [
    "### 1.3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QqIHv0pnczIs"
   },
   "source": [
    "# The following classes implement the MDP and a deterministic policy\n",
    "\n",
    "\n",
    "class StudentMDP(object):\n",
    "  \"\"\"\n",
    "Basic class implementing the MDP.\n",
    "  Use get_data() to obtain transition probabilities and rewards.\n",
    "  \"\"\"\n",
    "\n",
    "  ACTION_PARTY = 0\n",
    "  ACTION_STUDY = 1\n",
    "  STATE_HAPPY = 0\n",
    "  STATE_SAD = 1\n",
    "\n",
    "  def __init__(self):\n",
    "      self.data = [[[[], []], [[], []]], [[[], []], [[], []]]]\n",
    "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_PARTY][StudentMDP.STATE_HAPPY] = (1. , 10.)\n",
    "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_PARTY][StudentMDP.STATE_SAD] = (0., 0.)\n",
    "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_STUDY][StudentMDP.STATE_HAPPY] = (0.2, -10.)\n",
    "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_STUDY][StudentMDP.STATE_SAD] = (0.8, -10.)\n",
    "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_PARTY][StudentMDP.STATE_HAPPY] = (0.8, 40.)\n",
    "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_PARTY][StudentMDP.STATE_SAD] = (0.2, 40)\n",
    "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_STUDY][StudentMDP.STATE_HAPPY] = (0.8, 20)\n",
    "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_STUDY][StudentMDP.STATE_SAD] = (0.2, 20)\n",
    "\n",
    "  def get_data(self, state, action, state_prime):\n",
    "    # Computes the transition probabillities and the reward going from 'state'\n",
    "    # to 'state_prime' under 'action'.\n",
    "    # Eg.: p, r = get_data(StudentMDP.STATE_HAPPY, StudentMDP.ACTION_PARTY, StudentMDP.STATE_SAD)\n",
    "\n",
    "      if (state < 0 or state >= self.get_num_states()):\n",
    "          raise Exception(\"Invalid state\")\n",
    "      if (state_prime < 0 or state_prime >= self.get_num_states()):\n",
    "          raise Exception(\"Invalid state_prime\")\n",
    "      if (action < 0 or action >= self.get_num_actions()):\n",
    "          raise Exception(\"Invalid action\")\n",
    "      return self.data[state][action][state_prime]\n",
    "\n",
    "  def get_num_states(self):\n",
    "    # Number of states\n",
    "      return 2\n",
    "\n",
    "  def get_num_actions(self):\n",
    "    # Number of actions\n",
    "      return 2\n",
    "\n",
    "\n",
    "class DeterministicPolicy(object):\n",
    "  \"\"\"\n",
    "  Class describing a deterministic policy. Example:\n",
    "\n",
    "  policy = DeterministicPolicy(2, 2)\n",
    "  for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "    policy.set_action(state, StudentMDP.ACTION_PARTY)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_states, num_actions):\n",
    "      self.num_states = num_states\n",
    "      self.num_actions = num_actions\n",
    "      self.state_action_map = np.zeros(num_states, dtype=np.int32)\n",
    "\n",
    "  def get_action(self, state):\n",
    "      if (state < 0 or state >= self.num_states):\n",
    "          raise Exception(\"Invalid state\")\n",
    "      return self.state_action_map[state]\n",
    "\n",
    "  def set_action(self, state, action):\n",
    "      if (state < 0 or state >= self.num_states):\n",
    "          raise Exception(\"Invalid state\")\n",
    "      if (action < 0 or action >= self.num_actions):\n",
    "          raise Exception(\"Invalid action\")\n",
    "      self.state_action_map[state] = action"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCU4YP8h9yVE"
   },
   "source": [
    "### 1.3.1\n",
    "Write a function to compute the probability of being at   $s'$ after  $T$ steps, starting  from $s$, and following a deterministic policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F8Vj0gtx90xI"
   },
   "source": [
    "student = StudentMDP()\n",
    "num_states = student.get_num_states()\n",
    "num_actions = student.get_num_states()\n",
    "\n",
    "policy = DeterministicPolicy(num_states, num_actions)\n",
    "for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "    policy.set_action(state, StudentMDP.ACTION_PARTY)\n",
    "\n",
    "\n",
    "def probMDP(state, state_prime, T_steps):\n",
    "    if T_steps == 1:\n",
    "        prob_sum = 0\n",
    "        for action in range(num_actions):\n",
    "            # deterministic probability\n",
    "            policy_prob = 0\n",
    "            if action == 0:\n",
    "                policy_prob = 1\n",
    "            prob_sum = prob_sum + student.get_data(state, action, state_prime)[0]*policy_prob\n",
    "        return prob_sum\n",
    "    else:\n",
    "        prob_sum = 0\n",
    "        for statei in range(num_states):\n",
    "            for actionj in range(num_actions):\n",
    "                policy_prob = 0\n",
    "                if actionj == 0:\n",
    "                    policy_prob = 1\n",
    "                prob_sum = prob_sum + student.get_data(statei, actionj, state_prime)[0]*policy_prob*probMDP(state, statei, T_steps-1)\n",
    "        return prob_sum\n",
    "\n"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM-T5YVy-YwG"
   },
   "source": [
    "### 1.3.2\n",
    "\n",
    "Using this code compute\n",
    "\n",
    "\n",
    "1.   $P(S_1 =\\texttt{happy} \\mid S_0 = \\texttt{happy})=1.0$\n",
    "2.   $P(S_1 = \\texttt{happy} \\mid S_0 = \\texttt{sad})=0.8$\n",
    "3.   $P(S_2 = \\texttt{happy} \\mid S_0 = \\texttt{happy})=1.0$\n",
    "4.   $P(S_2 = \\texttt{happy} \\mid S_0 = \\texttt{sad})=0.96$\n",
    "5.   $P(S_3 = \\texttt{happy} \\mid S_0 = \\texttt{happy})=1.0$\n",
    "6.   $P(S_3 = \\texttt{happy} \\mid S_0 = \\texttt{sad})=0.992$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0vcC2q7k_bcD"
   },
   "source": [
    "# ------------\n",
    "print(probMDP(StudentMDP.STATE_HAPPY, StudentMDP.STATE_HAPPY, 1))\n",
    "print(probMDP(StudentMDP.STATE_SAD, StudentMDP.STATE_HAPPY, 1))\n",
    "print(probMDP(StudentMDP.STATE_HAPPY, StudentMDP.STATE_HAPPY, 2))\n",
    "print(probMDP(StudentMDP.STATE_SAD, StudentMDP.STATE_HAPPY, 2))\n",
    "print(probMDP(StudentMDP.STATE_HAPPY, StudentMDP.STATE_HAPPY, 3))\n",
    "print(probMDP(StudentMDP.STATE_SAD, StudentMDP.STATE_HAPPY, 3))\n",
    "# ------------"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8\n",
      "1.0\n",
      "0.9600000000000001\n",
      "1.0\n",
      "0.9920000000000001\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9kMsUzlAqdV"
   },
   "source": [
    "Compute $P(S_2 = \\texttt{happy} | S_0 = \\texttt{sad})$ using  (1.1) and compare with the code output.\n",
    "\n",
    "**Your answer:**\n",
    "![](ECE2795HW1%20Pr%201.3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion:\n",
    "Working out each time step allowed me to better understand the algorithm for computing the T-Step probabilities"
   ],
   "metadata": {
    "id": "hpfohA5HvuIM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajx00zPUdEc4"
   },
   "source": [
    "---\n",
    "# 2. Policy Improvement\n",
    "\n",
    "Proof below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaYpxi-lBdBk"
   },
   "source": [
    "## 2.1\n",
    "\n",
    "Prove $$\\mathbb{E}_{a \\sim \\pi^\\prime}[q_\\pi(s, a)] \\geq v_\\pi(s).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g24lWpNQB-Yi"
   },
   "source": [
    "## 2.2\n",
    "\n",
    "Prove $$Q_2 := \\mathbb{E}_{\\pi^\\prime}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})] \\geq Q_1.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PScn7fnPEOyr"
   },
   "source": [
    "## 2.3\n",
    "Prove\n",
    "$$Q_{T+1} \\geq Q_{T}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj3M4cMSE12a"
   },
   "source": [
    "## 2.4\n",
    "Conclude the proof by arguing that $Q_\\infty \\geq v_\\pi(s)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TLls0QbE_-6"
   },
   "source": [
    "Brief discussion\n",
    "---\n",
    "![](ECE2795HW1%20Pr%202.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7o_HIFmAM52"
   },
   "source": [
    "# 3. Policy Iteration\n",
    "\n",
    "\n",
    "## 3.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zwSQRWuj1sPr"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Complete this function.\n",
    "# Implements Policy Evaluation. The inputs are an object of class StudentMDP\n",
    "# and a policy to be evaluated. Returns a numpy array of size (num_states,)\n",
    "# with the value function for each state\n",
    "def PolicyEvaluation(mdp, policy, gamma=.9):\n",
    "    N = mdp.get_num_states()\n",
    "    value = np.zeros(N)\n",
    "\n",
    "    theta = 0.1\n",
    "\n",
    "    idx = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = 0\n",
    "        for state in range(N):\n",
    "            action = policy.get_action(state)\n",
    "            for next_state in range(N):\n",
    "                p, r = mdp.get_data(state, action, next_state)\n",
    "                V_new = V_new + p*(r + gamma*value[next_state])\n",
    "            pass\n",
    "            delta = max(delta, np.abs(V_new - value[state]))\n",
    "            value[state] = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "        idx = idx + 1\n",
    "    return value\n",
    "\n",
    "# Complete this function.\n",
    "# Implements Policy Improvement. The inputs are an MDP and a value function\n",
    "# (obtained by applying PolicyEvaluation()), computes the q-function for each state\n",
    "# and returns a new policy, greedy with respect to q.\n",
    "\n",
    "def PolicyImprovement(mdp, value_func, gamma=.9):\n",
    "\n",
    "    N = mdp.get_num_states()\n",
    "    M = mdp.get_num_actions()\n",
    "    policy = DeterministicPolicy(N, M)\n",
    "    for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "        policy.set_action(state, StudentMDP.ACTION_STUDY)\n",
    "\n",
    "    for state in range(N):\n",
    "        new_policy = policy\n",
    "        for action in range(M):\n",
    "            new_policy.set_action(state, np.argmax(PolicyEvaluation(mdp, policy)))\n",
    "        pass\n",
    "        # set policy to new policy\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# Complete this function\n",
    "# The input is two policies  old_policy and new_policy,\n",
    "# being both of the class DeterministicPolicy().\n",
    "# Returns True or False indicating if PolicyIteration should stop or not.\n",
    "def ShouldTerminate(mdp, old_policy, new_policy):\n",
    "    if (old_policy.state_action_map == new_policy.state_action_map).all():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def PolicyIteration(mdp, initial_policy, gamma=.9):\n",
    "    policy = initial_policy\n",
    "    value_data = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        value_func = PolicyEvaluation(mdp, policy, gamma)\n",
    "        value_data.append(value_func)\n",
    "        new_policy = PolicyImprovement(mdp, value_func, gamma)\n",
    "\n",
    "        should_terminate = ShouldTerminate(mdp, policy, new_policy)\n",
    "        if should_terminate:\n",
    "            break\n",
    "        policy = new_policy\n",
    "        idx += 1\n",
    "    return policy"
   ],
   "execution_count": 205,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC3OfwpIUK_2"
   },
   "source": [
    "## 3.2\n",
    "\n",
    "Apply this code for the student model of problem 1. Provide the following graphs:\n",
    "\n",
    "\n",
    "\n",
    "1.   Value function $v(s)$  for each state ($\\texttt{happy}$, $\\texttt{sad})$ as a function of  the excecition time $k$, being $k$ the number of runs of  Policy Evaluation.\n",
    "![](pr3.png)\n",
    "2.   Table with $\\pi(a|s)$ upon convergence.\n",
    "\n",
    "| state | action |\n",
    "| ------------- | ------------- |\n",
    "| Happy  | Study  |\n",
    "| Sad  | Party  |\n",
    "(for GitHub)\n",
    "\n",
    "3. Table with $v(s)$ upon convergence.\n",
    "\n",
    "| state | value |\n",
    "| ------------- | ------------- |\n",
    "| Happy  | -7.55  |\n",
    "| Sad  | 32.5  |\n",
    "(for GitHub)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UgVmmldqVOe4"
   },
   "source": [
    "student = StudentMDP()\n",
    "num_states = student.get_num_states()\n",
    "num_actions = student.get_num_states()\n",
    "\n",
    "policy = DeterministicPolicy(num_states, num_actions)\n",
    "for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "    policy.set_action(state, StudentMDP.ACTION_STUDY)\n",
    "\n",
    "policy_Pol_Iter = PolicyIteration(student, policy, gamma=.1)"
   ],
   "execution_count": 206,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:20: RuntimeWarning: overflow encountered in scalar add\n",
      "  V_new = V_new + p*(r + gamma*value[next_state])\n",
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:22: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  delta = max(delta, np.abs(V_new - value[state]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDil4ziFWVu9"
   },
   "source": [
    "From a computational point of view, can you think of a problem with this method?\n",
    "\n",
    "Discuss briefly:\n",
    "You need to compute the evaluation for each policy every iteration. If it doesn't converge quickly then you may be left waiting a long time until the solution converges."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "jrpHbXcnxFjy"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Im44CXKgqQI"
   },
   "source": [
    "# 4. Value iteration\n",
    "\n",
    "## 4.1\n",
    "Implement the Value Iteration algorithm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zD8Rxk4shRFY"
   },
   "source": [
    "def ValueIteration(mdp, initial_policy, gamma=.9):\n",
    "    N = mdp.get_num_states()\n",
    "    M = mdp.get_num_actions()\n",
    "\n",
    "    value = np.zeros(N)\n",
    "\n",
    "    theta = 0.1\n",
    "    # Value Iteration\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = 0\n",
    "        for state in range(N):\n",
    "            for action in range(M):\n",
    "                V_action = 0\n",
    "                for next_state in range(N):\n",
    "                    p, r = mdp.get_data(state, action, next_state)\n",
    "                    V_action = V_action + p*(r + gamma*value[next_state])\n",
    "                pass\n",
    "                V_new = max(V_new, V_action)\n",
    "            delta = max(delta, np.abs(V_new - value[state]))\n",
    "            value[state] = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Make the Policy\n",
    "    policy = initial_policy\n",
    "    for state in range(N):\n",
    "        Action_Eval = np.zeros(M)\n",
    "        for action in range(M):\n",
    "            for next_state in range(N):\n",
    "                p, r = mdp.get_data(state, action, next_state)\n",
    "                Action_Eval[action] = Action_Eval[action] + p*(r + gamma*value[next_state])\n",
    "        policy.set_action(state, np.argmax(Action_Eval))\n",
    "    return policy\n"
   ],
   "execution_count": 207,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsR1Zhge272Z"
   },
   "source": [
    "## 4.2\n",
    "\n",
    " Use your code for the MDP in problem 1. Provide the following graphs:\n",
    "\n",
    "\n",
    "1.   Value function $v(s)$  for each state ($\\texttt{happy}$, $\\texttt{sad})$ as a function of  the excecition time $k$, being $k$ the number of times the outer loop is run.\n",
    "![](pr3.png)\n",
    "2.   Table with $\\pi(a|s)$ upon convergence.\n",
    "\n",
    "| state | action |\n",
    "| ------------- | ------------- |\n",
    "| Happy  | Study  |\n",
    "| Sad  | Party  |\n",
    "(for GitHub)\n",
    "\n",
    "3. Table with $v(s)$ upon convergence.\n",
    "\n",
    "| state | value |\n",
    "| ------------- | ------------- |\n",
    "| Happy  | -7.55  |\n",
    "| Sad  | 32.5  |\n",
    "(for GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_KrgJxKH3Ia6"
   },
   "source": [
    "student = StudentMDP()\n",
    "num_states = student.get_num_states()\n",
    "num_actions = student.get_num_states()\n",
    "\n",
    "policy = DeterministicPolicy(num_states, num_actions)\n",
    "for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "    policy.set_action(state, StudentMDP.ACTION_STUDY)\n",
    "\n",
    "policy_Val_Iter = PolicyIteration(student, policy, gamma=.1)"
   ],
   "execution_count": 208,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:20: RuntimeWarning: overflow encountered in scalar add\n",
      "  V_new = V_new + p*(r + gamma*value[next_state])\n",
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:22: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  delta = max(delta, np.abs(V_new - value[state]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discuss briefly, comparing with PI in part 3.\n",
    "\n",
    "The policy is equivalent to that of part 3 except the runtime is significantly shorter"
   ],
   "metadata": {
    "id": "HU9xgeUq07z9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 Bellman equations"
   ],
   "metadata": {
    "id": "GFmjeJyt7IsW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 5.1 Write the Bellman equations for the student model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.     14.384]\n"
     ]
    }
   ],
   "source": [
    "def BellmanEquation(mdp, policy, gamma=.9):\n",
    "    N = mdp.get_num_states()\n",
    "    M = mdp.get_num_actions()\n",
    "\n",
    "    value = np.zeros(N)\n",
    "    for state in range(N):\n",
    "        action = policy.get_action(state)\n",
    "        for next_state in range(N):\n",
    "            p, r = mdp.get_data(state, action, next_state)\n",
    "            value[state] = value[state] + p*(r + gamma*value[next_state])\n",
    "\n",
    "    return value\n",
    "\n",
    "student = StudentMDP()\n",
    "num_states = student.get_num_states()\n",
    "num_actions = student.get_num_states()\n",
    "\n",
    "policy = DeterministicPolicy(num_states, num_actions)\n",
    "for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
    "    policy.set_action(state, StudentMDP.ACTION_STUDY)\n",
    "\n",
    "print(BellmanEquation(student, policy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 5.2 Write a piece of code to compute the Bellman error for this MDP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "student = StudentMDP()\n",
    "num_states = student.get_num_states()\n",
    "num_actions = student.get_num_states()\n",
    "\n",
    "# Bellman Equation before and after runnning another Policy Iteration\n",
    "print(BellmanEquation(student, policy_Pol_Iter))\n",
    "print(BellmanEquation(student, PolicyIteration(student, policy_Pol_Iter)))\n",
    "print(\"Error: \" + str(BellmanEquation(student, policy_Pol_Iter) - BellmanEquation(student, PolicyIteration(student, policy_Pol_Iter))))\n",
    "\n",
    "# Bellman Equation before and after runnning another Value Iteration\n",
    "print(BellmanEquation(student, policy_Val_Iter))\n",
    "print(BellmanEquation(student, ValueIteration(student, policy_Val_Iter)))\n",
    "print(\"Error: \" + str(BellmanEquation(student, policy_Val_Iter) - BellmanEquation(student, ValueIteration(student, policy_Val_Iter))))"
   ],
   "metadata": {
    "id": "eM6-xI2X7XqI"
   },
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.     37.264]\n",
      "[-10.     37.264]\n",
      "Error: [0. 0.]\n",
      "[-10.     37.264]\n",
      "[-10.     37.264]\n",
      "Error: [0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:20: RuntimeWarning: overflow encountered in scalar add\n",
      "  V_new = V_new + p*(r + gamma*value[next_state])\n",
      "C:\\Users\\ereij\\AppData\\Local\\Temp\\ipykernel_22984\\2473680647.py:22: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  delta = max(delta, np.abs(V_new - value[state]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Concluding remarks:\n",
    "This HW helped solidify my knowledge of MDP, value functions, and policies. It also helped me better understand the algorithms behind fundamental reinforcement learning problems. I think the most difficult part of the assignment was understanding the math syntax of the algorithms and equations, as well as working through the proofs, but after solving them I have gained a better understanding for then."
   ],
   "metadata": {
    "id": "iR7qQMkB1FwP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
